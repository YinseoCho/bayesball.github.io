[["index.html", "Bayesian Computation with R Scripts Chapter 1 Preface", " Bayesian Computation with R Scripts Jim Albert 2020-12-18 Chapter 1 Preface This book contains all of the R scripts and associated output for Chapters 2 through 10 of Bayesian Computation with R second edition. In these scripts, I have avoided the use of the attach() function and spaces have been added to increase readability. "],["introduction-to-bayesian-thinking.html", "Chapter 2 Introduction to Bayesian Thinking 2.1 Learning About the Proportion of Heavy Sleepers 2.2 Using a Discrete Prior 2.3 Using a Beta Prior 2.4 Using a Histogram Prior 2.5 Prediction", " Chapter 2 Introduction to Bayesian Thinking 2.1 Learning About the Proportion of Heavy Sleepers Want to learn about \\(p\\), the proportion of heavy sleepers. Take a sample of 27 students and 11 are heavy sleepers. 2.2 Using a Discrete Prior library(LearnBayes) The prior for \\(p\\): p &lt;- seq(0.05, 0.95, by = 0.1) prior &lt;- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior &lt;- prior / sum(prior) plot(p, prior, type = &quot;h&quot;, ylab=&quot;Prior Probability&quot;) The posterior for \\(p\\): data &lt;- c(11, 16) post &lt;- pdisc(p, prior, data) round(cbind(p, prior, post),2) ## p prior post ## [1,] 0.05 0.03 0.00 ## [2,] 0.15 0.18 0.00 ## [3,] 0.25 0.28 0.13 ## [4,] 0.35 0.25 0.48 ## [5,] 0.45 0.16 0.33 ## [6,] 0.55 0.07 0.06 ## [7,] 0.65 0.02 0.00 ## [8,] 0.75 0.00 0.00 ## [9,] 0.85 0.00 0.00 ## [10,] 0.95 0.00 0.00 library(lattice) PRIOR &lt;- data.frame(&quot;prior&quot;, p, prior) POST &lt;- data.frame(&quot;posterior&quot;, p, post) names(PRIOR) &lt;- c(&quot;Type&quot;, &quot;P&quot;, &quot;Probability&quot;) names(POST) &lt;- c(&quot;Type&quot;,&quot;P&quot;,&quot;Probability&quot;) data &lt;- rbind(PRIOR, POST) xyplot(Probability ~ P | Type, data=data, layout=c(1,2), type=&quot;h&quot;, lwd=3, col=&quot;black&quot;) 2.3 Using a Beta Prior Construct a beta prior for \\(p\\) by inputting two percentiles: quantile2 &lt;- list(p=.9, x=.5) quantile1 &lt;- list(p=.5, x=.3) (ab &lt;- beta.select(quantile1,quantile2)) ## [1] 3.26 7.19 Bayesian triplot: a &lt;- ab[1] b &lt;- ab[2] s &lt;- 11 f &lt;- 16 curve(dbeta(x, a + s, b + f), from=0, to=1, xlab=&quot;p&quot;, ylab=&quot;Density&quot;, lty=1, lwd=4) curve(dbeta(x, s + 1, f + 1), add=TRUE, lty=2, lwd=4) curve(dbeta(x, a, b), add=TRUE, lty=3, lwd=4) legend(.7, 4, c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;), lty=c(3, 2, 1), lwd=c(3, 3, 3)) Posterior summaries: 1 - pbeta(0.5, a + s, b + f) ## [1] 0.0690226 qbeta(c(0.05, 0.95), a + s, b + f) ## [1] 0.2555267 0.5133608 Simulating from posterior: ps &lt;- rbeta(1000, a + s, b + f) hist(ps, xlab=&quot;p&quot;) sum(ps &gt;= 0.5) / 1000 ## [1] 0.067 quantile(ps, c(0.05, 0.95)) ## 5% 95% ## 0.2470285 0.5114442 2.4 Using a Histogram Prior Beliefs about \\(p\\) are expressed by a histogram prior. Illustrate brute force method of computing the posterior. midpt &lt;- seq(0.05, 0.95, by = 0.1) prior &lt;- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior &lt;- prior / sum(prior) curve(histprior(x, midpt, prior), from=0, to=1, ylab=&quot;Prior density&quot;, ylim=c(0, .3)) s &lt;- 11 f &lt;- 16 curve(histprior(x,midpt,prior) * dbeta(x, s + 1, f + 1), from=0, to=1, ylab=&quot;Posterior density&quot;) p &lt;- seq(0, 1, length=500) post &lt;- histprior(p, midpt, prior) * dbeta(p, s + 1, f + 1) post &lt;- post / sum(post) ps &lt;- sample(p, replace = TRUE, prob = post) hist(ps, xlab=&quot;p&quot;, main=&quot;&quot;) 2.5 Prediction Want to predict the number of heavy sleepers in a future sample of 20. Discrete prior approach: p &lt;- seq(0.05, 0.95, by=.1) prior &lt;- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior &lt;- prior / sum(prior) m &lt;- 20 ys &lt;- 0:20 pred &lt;- pdiscp(p, prior, m, ys) cbind(0:20, pred) ## pred ## [1,] 0 2.030242e-02 ## [2,] 1 4.402694e-02 ## [3,] 2 6.894572e-02 ## [4,] 3 9.151046e-02 ## [5,] 4 1.064393e-01 ## [6,] 5 1.124487e-01 ## [7,] 6 1.104993e-01 ## [8,] 7 1.021397e-01 ## [9,] 8 8.932837e-02 ## [10,] 9 7.416372e-02 ## [11,] 10 5.851740e-02 ## [12,] 11 4.383668e-02 ## [13,] 12 3.107700e-02 ## [14,] 13 2.071698e-02 ## [15,] 14 1.284467e-02 ## [16,] 15 7.277453e-03 ## [17,] 16 3.667160e-03 ## [18,] 17 1.575535e-03 ## [19,] 18 5.381536e-04 ## [20,] 19 1.285179e-04 ## [21,] 20 1.584793e-05 Continuous prior approach: ab &lt;- c(3.26, 7.19) m &lt;- 20 ys &lt;- 0:20 pred &lt;- pbetap(ab, m, ys) Simulating predictive distribution: p &lt;- rbeta(1000, 3.26, 7.19) y &lt;- rbinom(1000, 20, p) table(y) ## y ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 16 54 66 81 89 103 101 135 91 77 54 43 34 31 12 3 7 1 2 freq &lt;- table(y) ys &lt;- as.integer(names(freq)) predprob &lt;- freq / sum(freq) plot(ys, predprob, type=&quot;h&quot;, xlab=&quot;y&quot;, ylab=&quot;Predictive Probability&quot;) dist &lt;- cbind(ys, predprob) Construction of a prediction interval: covprob &lt;- .9 discint(dist, covprob) ## $prob ## 12 ## 0.928 ## ## $set ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 1 2 3 4 5 6 7 8 9 10 11 12 "],["single-parameter-models.html", "Chapter 3 Single-Parameter Models 3.1 Normal Distribution with Known Mean but Unknown Variance 3.2 Estimating a Heart Transplant Mortality Rate 3.3 An Illustration of Bayesian Robustness 3.4 Mixtures of Conjugate Priors 3.5 A Bayesian Test of the Fairness of a Coin", " Chapter 3 Single-Parameter Models 3.1 Normal Distribution with Known Mean but Unknown Variance Assuming we have a sample {\\(y_j\\)} from a normal distribution with mean 0 and variance \\(\\sigma^2\\). Assuming the prior \\(g(\\sigma^2) \\propto 1/\\sigma^2\\), simulating from the posterior. library(LearnBayes) d &lt;- with(footballscores, favorite - underdog - spread) n &lt;- length(d) v &lt;- sum(d ^ 2) P &lt;- rchisq(1000, n) / v s &lt;- sqrt(1 / P) hist(s) quantile(s, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 13.14654 13.85587 14.62902 3.2 Estimating a Heart Transplant Mortality Rate Have a sample {\\(y_j\\)} from a Poisson(\\(e \\lambda\\)) distribution where the exposure \\(e\\) is known. Assigning \\(\\lambda\\) a gamma(\\(\\alpha, \\beta\\)) prior. Predictive density: alpha &lt;- 16; beta &lt;- 15174 yobs &lt;- 1; ex &lt;- 66 y &lt;- 0:10 lam &lt;- alpha / beta py &lt;- dpois(y, lam * ex) * dgamma(lam, shape = alpha, rate = beta) / dgamma(lam, shape = alpha + y, rate = beta + ex) cbind(y, round(py, 3)) ## y ## [1,] 0 0.933 ## [2,] 1 0.065 ## [3,] 2 0.002 ## [4,] 3 0.000 ## [5,] 4 0.000 ## [6,] 5 0.000 ## [7,] 6 0.000 ## [8,] 7 0.000 ## [9,] 8 0.000 ## [10,] 9 0.000 ## [11,] 10 0.000 Posterior density: lambdaA &lt;- rgamma(1000, shape = alpha + yobs, rate = beta + ex) Data from a different hospital: ex &lt;- 1767; yobs &lt;-4 y &lt;- 0:10 py &lt;- dpois(y, lam * ex) * dgamma(lam, shape = alpha, rate = beta) / dgamma(lam, shape = alpha + y, rate = beta + ex) cbind(y, round(py, 3)) ## y ## [1,] 0 0.172 ## [2,] 1 0.286 ## [3,] 2 0.254 ## [4,] 3 0.159 ## [5,] 4 0.079 ## [6,] 5 0.033 ## [7,] 6 0.012 ## [8,] 7 0.004 ## [9,] 8 0.001 ## [10,] 9 0.000 ## [11,] 10 0.000 lambdaB &lt;- rgamma(1000, shape = alpha + yobs, rate = beta + ex) Prior and posteriors for two hospitals: par(mfrow = c(2, 1)) plot(density(lambdaA), main=&quot;HOSPITAL A&quot;, xlab=&quot;lambdaA&quot;, lwd=3) curve(dgamma(x, shape = alpha, rate = beta), add=TRUE) legend(&quot;topright&quot;,legend=c(&quot;prior&quot;,&quot;posterior&quot;), lwd=c(1,3)) plot(density(lambdaB), main=&quot;HOSPITAL B&quot;, xlab=&quot;lambdaB&quot;, lwd=3) curve(dgamma(x, shape = alpha, rate = beta), add=TRUE) legend(&quot;topright&quot;,legend=c(&quot;prior&quot;,&quot;posterior&quot;), lwd=c(1,3)) 3.3 An Illustration of Bayesian Robustness Assuming normal sampling (known standard deviation), compare the use of two priors on the mean \\(\\mu\\). quantile1 &lt;- list(p=.5, x=100) quantile2 &lt;- list(p=.95, x=120) normal.select(quantile1, quantile2) ## $mu ## [1] 100 ## ## $sigma ## [1] 12.15914 mu &lt;- 100 tau &lt;- 12.16 sigma &lt;- 15 n &lt;- 4 se &lt;- sigma / sqrt(4) ybar &lt;- c(110, 125, 140) tau1 &lt;- 1 / sqrt(1 / se ^ 2 + 1 / tau ^ 2) mu1 &lt;- (ybar / se ^ 2 + mu / tau ^ 2) * tau1 ^ 2 summ1 &lt;- cbind(ybar, mu1, tau1) summ1 ## ybar mu1 tau1 ## [1,] 110 107.2442 6.383469 ## [2,] 125 118.1105 6.383469 ## [3,] 140 128.9768 6.383469 Compare two possible priors for \\(\\mu\\): tscale &lt;- 20 / qt(0.95, 2) tscale ## [1] 6.849349 par(mfrow=c(1, 1)) curve(1 / tscale * dt((x - mu) / tscale, 2), from=60, to=140, xlab=&quot;theta&quot;, ylab=&quot;Prior Density&quot;) curve(dnorm(x, mean=mu, sd=tau), add=TRUE, lwd=3) legend(&quot;topright&quot;, legend=c(&quot;t density&quot;, &quot;normal density&quot;), lwd=c(1,3)) norm.t.compute &lt;- function(ybar){ theta &lt;- seq(60, 180, length = 500) like &lt;- dnorm(theta, mean=ybar, sd=sigma/sqrt(n)) prior &lt;- dt((theta - mu) / tscale, 2) post &lt;- prior * like post &lt;- post / sum(post) m &lt;- sum(theta * post) s &lt;- sqrt(sum(theta ^ 2 * post) - m ^ 2) c(ybar, m, s) } summ2 &lt;- t(sapply(c(110, 125, 140), norm.t.compute)) dimnames(summ2)[[2]] &lt;- c(&quot;ybar&quot;, &quot;mu1 t&quot;, &quot;tau1 t&quot;) summ2 ## ybar mu1 t tau1 t ## [1,] 110 105.2921 5.841676 ## [2,] 125 118.0841 7.885174 ## [3,] 140 135.4134 7.973498 cbind(summ1, summ2) ## ybar mu1 tau1 ybar mu1 t tau1 t ## [1,] 110 107.2442 6.383469 110 105.2921 5.841676 ## [2,] 125 118.1105 6.383469 125 118.0841 7.885174 ## [3,] 140 128.9768 6.383469 140 135.4134 7.973498 Compare two posterior densities: theta &lt;- seq(60, 180, length=500) normpost &lt;- dnorm(theta, mu1[3], tau1) normpost &lt;- normpost / sum(normpost) plot(theta, normpost, type=&quot;l&quot;, lwd=3, ylab=&quot;Posterior Density&quot;) like &lt;- dnorm(theta, mean=140, sd=sigma / sqrt(n)) prior &lt;- dt((theta - mu) / tscale, 2) tpost &lt;- prior * like / sum(prior * like) lines(theta, tpost) legend(&quot;topright&quot;, legend=c(&quot;t prior&quot;, &quot;normal prior&quot;), lwd=c(1,3)) 3.4 Mixtures of Conjugate Priors Use a mixture of beta curves to reflect beliefs that a particular coin is biased. curve(.5 * dbeta(x, 6, 14) + .5 * dbeta(x, 14, 6), from=0, to=1, xlab=&quot;P&quot;, ylab=&quot;Density&quot;) probs &lt;- c(.5, .5) beta.par1 &lt;- c(6, 14) beta.par2 &lt;- c(14, 6) betapar &lt;- rbind(beta.par1, beta.par2) data &lt;- c(7, 3) post &lt;- binomial.beta.mix(probs, betapar, data) post ## $probs ## beta.par1 beta.par2 ## 0.09269663 0.90730337 ## ## $betapar ## [,1] [,2] ## beta.par1 13 17 ## beta.par2 21 9 Compare prior and posterior densities for the probability coin lands heads. curve(post$probs[1] * dbeta(x,13,17) + post$probs[2] * dbeta(x,21,9), from=0, to=1, lwd=3, xlab=&quot;P&quot;, ylab=&quot;DENSITY&quot;) curve(.5 * dbeta(x, 6, 12) + .5 * dbeta(x, 12, 6), 0, 1, add=TRUE) legend(&quot;topleft&quot;, legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), lwd=c(1, 3)) 3.5 A Bayesian Test of the Fairness of a Coin Testing if a coin is fair. Observe 5 heads in 20 flips. P-value calculation: pbinom(5, 20, 0.5) ## [1] 0.02069473 Bayesian test of fairness using a mixture prior. n &lt;- 20 y &lt;- 5 a &lt;- 10 p &lt;- 0.5 m1 &lt;- dbinom(y, n, p) * dbeta(p, a, a) / dbeta(p, a + y, a + n - y) lambda &lt;- dbinom(y, n, p) / (dbinom(y, n, p) + m1) lambda ## [1] 0.2802215 pbetat(p, .5, c(a, a), c(y, n - y)) ## $bf ## [1] 0.3893163 ## ## $post ## [1] 0.2802215 prob.fair &lt;- function(log.a){ a &lt;- exp(log.a) m2 &lt;- dbinom(y, n, p) * dbeta(p, a, a) / dbeta(p, a + y, a + n - y) dbinom(y, n, p) / (dbinom(y, n, p) + m2) } n &lt;- 20; y &lt;- 5; p &lt;- 0.5 curve(prob.fair(x), from = -4, to = 5, xlab=&quot;log a&quot;, ylab=&quot;Prob(coin is fair)&quot;, lwd=2) n &lt;- 20; y &lt;- 5 a &lt;- 10; p &lt;- .5 m2 &lt;- 0 for (k in 0:y){ m2 &lt;- m2 + dbinom(k, n, p) * dbeta(p, a, a) / dbeta(p, a + k, a + n - k) } lambda &lt;- pbinom(y, n, p) / (pbinom(y, n, p) + m2) lambda ## [1] 0.2184649 "],["multiparameter-models.html", "Chapter 4 Multiparameter Models 4.1 Normal Data with Both Parameters Unknown 4.2 A Multinomial Model 4.3 A Bioassay Experiment 4.4 Comparing Two Proportions", " Chapter 4 Multiparameter Models 4.1 Normal Data with Both Parameters Unknown Illustrates exact posterior sampling of (\\(\\mu, \\sigma^2\\)) for normal sampling with a noninformative prior. library(LearnBayes) mycontour(normchi2post, c(220, 330, 500, 9000), marathontimes$time, xlab=&quot;mean&quot;, ylab=&quot;variance&quot;) S &lt;- with(marathontimes, sum((time - mean(time))^2)) n &lt;- length(marathontimes$time) sigma2 &lt;- S / rchisq(1000, n - 1) mu &lt;- rnorm(1000, mean = mean(marathontimes$time), sd = sqrt(sigma2) / sqrt(n)) mycontour(normchi2post, c(220, 330, 500, 9000), marathontimes$time, xlab=&quot;mean&quot;, ylab=&quot;variance&quot;) points(mu, sigma2) quantile(mu, c(0.025, 0.975)) ## 2.5% 97.5% ## 255.8784 300.8766 quantile(sqrt(sigma2), c(0.025, 0.975)) ## 2.5% 97.5% ## 38.16360 72.06066 4.2 A Multinomial Model Multinomial data and a uniform prior placed on the proportions. Sampling from the Dirichlet posterior distribution. alpha &lt;- c(728, 584, 138) theta &lt;- rdirichlet(1000, alpha) hist(theta[, 1] - theta[, 2], main=&quot;&quot;) Considers posterior distribution of Obama electoral votes for the 2008 presidential election. prob.Obama &lt;- function(j){ p &lt;- with(election.2008, rdirichlet(5000, 500 * c(M.pct[j], O.pct[j], 100 - M.pct[j] - O.pct[j]) / 100 + 1)) mean(p[, 2] &gt; p[, 1]) } Obama.win.probs &lt;- sapply(1 : 51, prob.Obama) sim.election &lt;- function(){ winner &lt;- rbinom(51, 1, Obama.win.probs) sum(election.2008$EV * winner) } sim.EV &lt;- replicate(1000, sim.election()) hist(sim.EV, min(sim.EV) : max(sim.EV), col=&quot;blue&quot;) abline(v=365, lwd=3) # Obama received 365 votes text(375, 30, &quot;Actual \\n Obama \\n total&quot;) 4.3 A Bioassay Experiment Bayesian fitting of a logistic model using data from a dose-response experiment. x &lt;- c(-0.86, -0.3, -0.05, 0.73) n &lt;- c(5, 5, 5, 5) y &lt;- c(0, 1, 3, 5) data &lt;- cbind(x, n, y) Traditional logistic model fit. glmdata &lt;- cbind(y, n - y) results &lt;- glm(glmdata ~ x, family = binomial) summary(results) ## ## Call: ## glm(formula = glmdata ~ x, family = binomial) ## ## Deviance Residuals: ## 1 2 3 4 ## -0.17236 0.08133 -0.05869 0.12237 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.8466 1.0191 0.831 0.406 ## x 7.7488 4.8728 1.590 0.112 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 15.791412 on 3 degrees of freedom ## Residual deviance: 0.054742 on 2 degrees of freedom ## AIC: 7.9648 ## ## Number of Fisher Scoring iterations: 7 Illustration of a conditional means prior. When x = -.7, median and 90th percentile of p are (.2,.4). When x = +.6, median and 90th percentile of p are (.8, .95) a1.b1 &lt;- beta.select(list(p=.5, x=.2), list(p=.9, x=.5)) a2.b2 &lt;- beta.select(list(p=.5, x=.8), list(p=.9, x=.98)) prior &lt;- rbind(c(-0.7, 4.68, 1.12), c(0.6, 2.10, 0.74)) data.new &lt;- rbind(data, prior) Plot prior. plot(c(-1,1), c(0, 1), type=&quot;n&quot;, xlab=&quot;Dose&quot;, ylab=&quot;Prob(death)&quot;) lines(-0.7 * c(1, 1), qbeta(c(.25, .75), a1.b1[1], a1.b1[2]), lwd=4) lines(0.6 * c(1, 1), qbeta(c(.25, .75), a2.b2[1], a2.b2[2]), lwd=4) points(c(-0.7, 0.6), qbeta(.5, c(a1.b1[1], a2.b2[1]), c(a1.b1[2], a2.b2[2])), pch=19, cex=2) text(-0.3, .2, &quot;Beta(1.12, 3.56)&quot;) text(.2, .8, &quot;Beta(2.10, 0.74)&quot;) response &lt;- rbind(a1.b1, a2.b2) x &lt;- c(-0.7, 0.6) fit &lt;- glm(response ~ x, family = binomial) ## Warning in eval(family$initialize): non-integer counts in a binomial glm! curve(exp(fit$coef[1] + fit$coef[2] * x) / (1 + exp(fit$coef[1] + fit$coef[2] * x)), add=T) Posterior of regression coefficients. mycontour(logisticpost, c(-3, 3, -1, 9), data.new, xlab=&quot;beta0&quot;, ylab=&quot;beta1&quot;) mycontour(logisticpost, c(-3, 3, -1, 9), data.new, xlab=&quot;beta0&quot;, ylab=&quot;beta1&quot;) s &lt;- simcontour(logisticpost, c(-2, 3, -1, 11), data.new, 1000) points(s) plot(density(s$y), xlab=&quot;beta1&quot;) Estimation of LD50 parameter. theta &lt;- -s$x / s$y hist(theta, xlab=&quot;LD-50&quot;, breaks=20, xlim = c(-1, 1.5)) quantile(theta, c(.025, .975)) ## 2.5% 97.5% ## -0.3248469 0.4748061 4.4 Comparing Two Proportions Using Howard’s dependent prior for two proportions. Graph of the prior. sigma &lt;- c(2, 1, .5, .25) plo &lt;- .0001; phi &lt;- .9999 par(mfrow=c(2, 2)) for (i in 1:4){ mycontour(howardprior, c(plo, phi, plo, phi), c(1, 1, 1, 1, sigma[i]), main=paste(&quot;sigma=&quot;, as.character(sigma[i])), xlab=&quot;p1&quot;, ylab=&quot;p2&quot;) } Graphs of the posterior. sigma &lt;- c(2, 1, .5, .25) par(mfrow=c(2, 2)) for (i in 1:4){ mycontour(howardprior, c(plo, phi, plo, phi), c(1 + 3, 1 + 15, 1 + 7, 1 + 5, sigma[i]), main=paste(&quot;sigma=&quot;, as.character(sigma[i])), xlab=&quot;p1&quot;, ylab=&quot;p2&quot;) lines(c(0, 1), c(0, 1)) } s &lt;- simcontour(howardprior, c(plo, phi, plo, phi), c(1 + 3, 1 + 15, 1 + 7, 1 + 5, 2), 1000) sum(s$x &gt; s$y) / 1000 ## [1] 0.012 "],["introduction-to-bayesian-computation.html", "Chapter 5 Introduction to Bayesian Computation 5.1 A Beta-Binomial Model for Overdispersion 5.2 Approximations Based on Posterior Modes 5.3 Monte Carlo Method for Computing Integrals 5.4 Rejection Sampling 5.5 Importance Sampling 5.6 Sampling Importance Resampling", " Chapter 5 Introduction to Bayesian Computation 5.1 A Beta-Binomial Model for Overdispersion library(LearnBayes) First consider posterior of \\((\\eta, K)\\). mycontour(betabinexch0, c(.0001, .003, 1, 20000), cancermortality, xlab=&quot;eta&quot;, ylab=&quot;K&quot;) Instead look at posterior of \\((\\log \\frac{\\eta}{1-\\eta}, \\log I\\). mycontour(betabinexch, c(-8, -4.5, 3, 16.5), cancermortality, xlab=&quot;logit eta&quot;, ylab=&quot;log K&quot;) 5.2 Approximations Based on Posterior Modes fit &lt;- laplace(betabinexch, c(-7, 6), cancermortality) fit ## $mode ## [1] -6.819793 7.576111 ## ## $var ## [,1] [,2] ## [1,] 0.07896568 -0.1485087 ## [2,] -0.14850874 1.3483208 ## ## $int ## [1] -570.7743 ## ## $converge ## [1] TRUE npar &lt;- list(m=fit$mode, v=fit$var) mycontour(lbinorm, c(-8, -4.5, 3, 16.5), npar, xlab=&quot;logit eta&quot;, ylab=&quot;log K&quot;) se &lt;- sqrt(diag(fit$var)) fit$mode - 1.645 * se ## [1] -7.282052 5.665982 fit$mode + 1.645 * se ## [1] -6.357535 9.486239 5.3 Monte Carlo Method for Computing Integrals Illustration of a simple estimate of an integral by Monte Carlo. p &lt;- rbeta(1000, 14.26, 23.19) est &lt;- mean(p ^ 2) se &lt;- sd(p ^ 2) / sqrt(1000) c(est,se) ## [1] 0.150436813 0.001985356 5.4 Rejection Sampling Using rejection sampling for the overdispersion posterior with a multivariate t proposal density. fit &lt;- laplace(betabinexch, c(-7, 6), cancermortality) betabinT &lt;- function(theta, datapar){ data &lt;- datapar$data tpar &lt;-datapar$par d &lt;- betabinexch(theta,data) - dmt(theta, mean=c(tpar$m), S=tpar$var, df=tpar$df, log=TRUE) d } tpar &lt;- list(m=fit$mode, var=2 * fit$var, df=4) datapar &lt;- list(data=cancermortality, par=tpar) start &lt;- c(-6.9, 12.4) fit1 &lt;- laplace(betabinT, start, datapar) fit1$mode ## [1] -6.888963 12.421993 betabinT(fit1$mode, datapar) ## [1] -569.2829 theta &lt;- rejectsampling(betabinexch, tpar, -569.2813, 10000, cancermortality) dim(theta) ## [1] 2438 2 mycontour(betabinexch, c(-8, -4.5, 3, 16.5), cancermortality, xlab=&quot;logit eta&quot;, ylab=&quot;log K&quot;) points(theta[,1],theta[,2]) 5.5 Importance Sampling fit &lt;- laplace(betabinexch, c(-7, 6), cancermortality) Posterior density of \\(\\log K\\)$ conditional on a value of \\(\\eta\\). betabinexch.cond &lt;- function (log.K, data){ eta &lt;- exp(-6.818793) / (1 + exp(-6.818793)) K &lt;- exp(log.K) y &lt;- data[, 1] n &lt;- data[, 2] N &lt;- length(y) logf &lt;- 0 * log.K for (j in 1:length(y)){ logf = logf + lbeta(K * eta + y[j], K * (1 - eta) + n[j] - y[j]) - lbeta(K * eta, K * (1 - eta)) } val &lt;- logf + log.K - 2 * log(1 + K) exp(val-max(val)) } Illustrate different choices of importance sampler. I &lt;- integrate(betabinexch.cond, 2, 16, cancermortality) par(mfrow=c(2, 2)) curve(betabinexch.cond(x, cancermortality) / I$value, from=3, to=16, ylab=&quot;Density&quot;, xlab=&quot;log K&quot;, lwd=3, main=&quot;Densities&quot;) curve(dnorm(x, 8, 2), add=TRUE) legend(&quot;topright&quot;, legend=c(&quot;Exact&quot;, &quot;Normal&quot;), lwd=c(3, 1)) curve(betabinexch.cond(x, cancermortality) / I$value / dnorm(x, 8, 2), from=3, to=16, ylab=&quot;Weight&quot;, xlab=&quot;log K&quot;, main=&quot;Weight = g/p&quot;) curve(betabinexch.cond(x, cancermortality) /I$value, from=3, to=16, ylab=&quot;Density&quot;, xlab=&quot;log K&quot;, lwd=3, main=&quot;Densities&quot;) curve(1 / 2 * dt(x - 8, df=2), add=TRUE) legend(&quot;topright&quot;, legend=c(&quot;Exact&quot;, &quot;T(2)&quot;), lwd=c(3, 1)) curve(betabinexch.cond(x, cancermortality) / I$value / (1 / 2 * dt(x - 8, df=2)), from=3, to=16, ylab=&quot;Weight&quot;, xlab=&quot;log K&quot;, main=&quot;Weight = g/p&quot;) tpar &lt;- list(m=fit$mode, var=2 * fit$var, df=4) myfunc &lt;- function(theta){ return(theta[2]) } s &lt;- impsampling(betabinexch, tpar, myfunc, 10000, cancermortality) cbind(s$est, s$se) ## [,1] [,2] ## [1,] 7.926348 0.01891307 5.6 Sampling Importance Resampling Illustrate using the SIR algorithm for the beta-binomial density with a multivariate t proposal density. fit &lt;- laplace(betabinexch, c(-7, 6), cancermortality) tpar &lt;- list(m=fit$mode, var=2 * fit$var, df=4) theta.s &lt;- sir(betabinexch, tpar, 10000, cancermortality) Use SIR to examine the sensitivity of the posterior inference to removal of individual observations. S &lt;- bayes.influence(theta.s, cancermortality) plot(c(0, 0, 0), S$summary, type=&quot;b&quot;, lwd=3, xlim=c(-1, 21), ylim=c(5, 11), xlab=&quot;Observation removed&quot;, ylab=&quot;log K&quot;) for (i in 1:20){ lines(c(i, i, i), S$summary.obs[i, ], type=&quot;b&quot;) } "],["markov-chain-monte-carlo-methods.html", "Chapter 6 Markov Chain Monte Carlo Methods 6.1 Introduction to Discrete Markov Chains 6.2 Learning about a Normal Population from Grouped Data 6.3 Example of Output Analysis 6.4 Modeling Data with Cauchy Errors 6.5 Analysis of the Stanford Heart Transplant Data", " Chapter 6 Markov Chain Monte Carlo Methods 6.1 Introduction to Discrete Markov Chains Illustration of sampling from a random walk distribution. P &lt;- matrix(c(.5, .5, 0, 0, 0, 0, .25, .5, .25, 0, 0, 0, 0, .25, .5, .25, 0, 0, 0, 0, .25, .5, .25, 0, 0, 0, 0, .25, .5, .25, 0, 0, 0, 0, .5, .5), nrow=6, ncol=6, byrow=TRUE) P ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.50 0.50 0.00 0.00 0.00 0.00 ## [2,] 0.25 0.50 0.25 0.00 0.00 0.00 ## [3,] 0.00 0.25 0.50 0.25 0.00 0.00 ## [4,] 0.00 0.00 0.25 0.50 0.25 0.00 ## [5,] 0.00 0.00 0.00 0.25 0.50 0.25 ## [6,] 0.00 0.00 0.00 0.00 0.50 0.50 s &lt;- array(0, c(50000, 1)) s[1] &lt;- 3 for (j in 2:50000){ s[j] &lt;- sample(1:6, size=1, prob=P[s[j - 1],]) } m &lt;- c(500, 2000, 8000, 50000) for (i in 1:4){ print(table(s[1:m[i]]) / m[i]) } ## ## 1 2 3 4 5 6 ## 0.102 0.224 0.208 0.186 0.236 0.044 ## ## 1 2 3 4 5 6 ## 0.1075 0.2115 0.2110 0.2180 0.1765 0.0755 ## ## 1 2 3 4 5 6 ## 0.095250 0.189875 0.219000 0.215875 0.193875 0.086125 ## ## 1 2 3 4 5 6 ## 0.09686 0.19760 0.20490 0.20618 0.19830 0.09616 w &lt;- matrix(c(.1, .2, .2, .2, .2, .1), nrow=1, ncol=6) w %*% P ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.1 0.2 0.2 0.2 0.2 0.1 6.2 Learning about a Normal Population from Grouped Data Have normally distributed data where the data is observed in grouped form. Consider the posterior of \\((\\mu, \\log \\sigma)\\). d &lt;- list(int.lo=c(-Inf, seq(66, 74, by=2)), int.hi=c(seq(66, 74, by=2), Inf), f=c(14, 30, 49, 70, 33, 15)) y &lt;- c(rep(65,14), rep(67,30), rep(69,49), rep(71,70), rep(73,33), rep(75,15)) mean(y) ## [1] 70.16588 log(sd(y)) ## [1] 0.9504117 First obtain normal approximation to posterior. start &lt;- c(70, 1) fit &lt;- laplace(groupeddatapost, start, d) fit ## $mode ## [1] 70.169880 0.973644 ## ## $var ## [,1] [,2] ## [1,] 3.534713e-02 3.520776e-05 ## [2,] 3.520776e-05 3.146470e-03 ## ## $int ## [1] -350.6305 ## ## $converge ## [1] TRUE Now use a Metropolis (random walk) MCMC algorithm. modal.sds &lt;- sqrt(diag(fit$var)) proposal &lt;- list(var=fit$var, scale=2) fit2 &lt;- rwmetrop(groupeddatapost, proposal, start, 10000, d) fit2$accept ## [1] 0.2908 post.means &lt;- apply(fit2$par, 2, mean) post.sds &lt;- apply(fit2$par, 2, sd) cbind(c(fit$mode), modal.sds) ## modal.sds ## [1,] 70.169880 0.18800834 ## [2,] 0.973644 0.05609341 cbind(post.means, post.sds) ## post.means post.sds ## [1,] 70.1636783 0.18672292 ## [2,] 0.9811132 0.05767941 mycontour(groupeddatapost, c(69, 71, .6, 1.3), d, xlab=&quot;mu&quot;,ylab=&quot;log sigma&quot;) points(fit2$par[5001:10000, 1], fit2$par[5001:10000, 2]) 6.3 Example of Output Analysis Illustrate MCMC diagnositics for different Metropolis chains with different proposal widths. d &lt;- list(int.lo=c(-Inf, seq(66, 74, by=2)), int.hi=c(seq(66, 74, by=2), Inf), f=c(14, 30, 49, 70, 33, 15)) library(coda) library(lattice) start &lt;- c(70,1) fit &lt;- laplace(groupeddatapost, start, d) start &lt;- c(65,1) proposal &lt;- list(var=fit$var, scale=0.2) bayesfit &lt;- rwmetrop(groupeddatapost, proposal, start, 10000, d) dimnames(bayesfit$par)[[2]] &lt;- c(&quot;mu&quot;, &quot;log sigma&quot;) xyplot(mcmc(bayesfit$par[-c(1:2000), ]), col=&quot;black&quot;) par(mfrow=c(2, 1)) autocorr.plot(mcmc(bayesfit$par[-c(1:2000), ]), auto.layout=FALSE) summary(mcmc(bayesfit$par[-c(1:2000), ])) ## ## Iterations = 1:8000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 8000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## mu 70.1880 0.19752 0.0022083 0.025874 ## log sigma 0.9709 0.05422 0.0006062 0.006305 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## mu 69.8124 70.0563 70.1806 70.31 70.588 ## log sigma 0.8628 0.9342 0.9723 1.01 1.071 batchSE(mcmc(bayesfit$par[-c(1:2000), ]), batchSize=50) ## mu log sigma ## 0.013983542 0.003739656 start &lt;- c(70,1) proposal &lt;- list(var=fit$var, scale=2.0) bayesfit &lt;- rwmetrop(groupeddatapost, proposal, start, 10000, d) dimnames(bayesfit$par)[[2]] &lt;- c(&quot;mu&quot;, &quot;log sigma&quot;) sim.parameters &lt;- mcmc(bayesfit$par[-c(1:2000), ]) xyplot(mcmc(bayesfit$par[-c(1:2000), ]), col=&quot;black&quot;) par(mfrow=c(2,1)) autocorr.plot(sim.parameters,auto.layout=FALSE) summary(sim.parameters) ## ## Iterations = 1:8000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 8000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## mu 70.165 0.18467 0.0020646 0.005728 ## log sigma 0.982 0.05744 0.0006422 0.001770 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## mu 69.8097 70.0346 70.1650 70.289 70.542 ## log sigma 0.8715 0.9416 0.9817 1.021 1.101 batchSE(sim.parameters, batchSize=50) ## mu log sigma ## 0.005387819 0.001751675 6.4 Modeling Data with Cauchy Errors Assuming data that is sampled from a Cauchy density with a noninformative prior placed on the location and scale parameters. mean(darwin$difference) ## [1] 21.66667 log(sd(darwin$difference)) ## [1] 3.65253 First illustrate normal approximation. laplace(cauchyerrorpost, c(21.6, 3.6), darwin$difference) ## $mode ## [1] 24.701745 2.772619 ## ## $var ## [,1] [,2] ## [1,] 34.9600525 0.3672899 ## [2,] 0.3672899 0.1378279 ## ## $int ## [1] -73.2404 ## ## $converge ## [1] TRUE laplace(cauchyerrorpost, .1 * c(21.6,3.6), darwin$difference)$mode ## [1] 24.698151 2.772345 c(24.7 - 4 * sqrt(34.96), 24.7 + 4 * sqrt(34.96)) ## [1] 1.049207 48.350793 c(2.77 - 4 * sqrt(.138), 2.77 + 4 * sqrt(.138)) ## [1] 1.284066 4.255934 mycontour(cauchyerrorpost, c(-10, 60, 1, 4.5), darwin$difference, xlab=&quot;mu&quot;, ylab=&quot;log sigma&quot;) fitlaplace &lt;- laplace(cauchyerrorpost, c(21.6, 3.6), darwin$difference) mycontour(lbinorm, c(-10, 60, 1, 4.5), list(m=fitlaplace$mode, v=fitlaplace$var), xlab=&quot;mu&quot;,ylab=&quot;log sigma&quot;) Next illustrate random walk Metropolis. proposal &lt;- list(var=fitlaplace$var, scale=2.5) start &lt;- c(20, 3) m &lt;- 1000 s &lt;- rwmetrop(cauchyerrorpost, proposal, start, m, darwin$difference) mycontour(cauchyerrorpost, c(-10, 60, 1, 4.5), darwin$difference, xlab=&quot;mu&quot;, ylab=&quot;log sigma&quot;) points(s$par[,1], s$par[,2]) fitgrid &lt;- simcontour(cauchyerrorpost, c(-10,60,1,4.5), darwin$difference, 50000) proposal &lt;- list(var=fitlaplace$var, scale=2.5) start=c(20, 3) fitrw=rwmetrop(cauchyerrorpost, proposal, start, 50000, darwin$difference) Illustrate metropolis-hastings independence chain. proposal2 &lt;- list(var=fitlaplace$var, mu=t(fitlaplace$mode)) fitindep &lt;- indepmetrop(cauchyerrorpost, proposal2, start, 50000, darwin$difference) Illustrate metropolis-within-Gibbs. fitgibbs &lt;- gibbs(cauchyerrorpost, start, 50000, c(12,.75), darwin$difference) apply(fitrw$par,2,mean) ## [1] 25.696204 2.841003 apply(fitrw$par,2,sd) ## [1] 7.1026378 0.3778886 6.5 Analysis of the Stanford Heart Transplant Data Using a Pareto model to analyze heart transplant data. Laplace fit. start &lt;- c(0, 3, -1) laplacefit &lt;- laplace(transplantpost, start, stanfordheart) laplacefit ## $mode ## [1] -0.09210954 3.38385249 -0.72334008 ## ## $var ## [,1] [,2] [,3] ## [1,] 0.172788525 -0.009282308 -0.04995160 ## [2,] -0.009282308 0.214737054 0.09301323 ## [3,] -0.049951602 0.093013230 0.06891796 ## ## $int ## [1] -376.2504 ## ## $converge ## [1] TRUE Random walk metropolis. proposal &lt;- list(var=laplacefit$var, scale=2) s &lt;- rwmetrop(transplantpost, proposal, start, 10000, stanfordheart) s$accept ## [1] 0.1893 par(mfrow=c(2,2)) tau &lt;- exp(s$par[,1]) plot(density(tau), main=&quot;TAU&quot;) lambda &lt;- exp(s$par[,2]) plot(density(lambda), main=&quot;LAMBDA&quot;) p &lt;- exp(s$par[,3]) plot(density(p), main=&quot;P&quot;) apply(exp(s$par), 2, quantile, c(.05, .5, .95)) ## [,1] [,2] [,3] ## 5% 0.4880657 13.43615 0.3149892 ## 50% 0.9422897 29.55993 0.4807512 ## 95% 1.9776292 64.32047 0.7666747 par(mfrow=c(1, 1)) t &lt;- seq(1, 240) p5 &lt;- 0*t p50 &lt;- 0 * t p95 &lt;- 0 * t for (j in 1:240){ S &lt;- (lambda / (lambda + t[j])) ^ p q &lt;- quantile(S, c(.05, .5, .95)) p5[j] &lt;- q[1] p50[j] &lt;- q[2] p95[j] &lt;- q[3] } Estimating a patient’s survival curve. plot(t, p50, type=&quot;l&quot;, ylim=c(0,1), ylab=&quot;Prob(Survival)&quot;, xlab=&quot;time&quot;) lines(t, p5, lty=2) lines(t, p95, lty=2) "],["hierarchical-modeling.html", "Chapter 7 Hierarchical Modeling 7.1 Introduction to Hierarchical Modeling 7.2 Individual or Combined Estimates 7.3 Equal Mortality Rates? 7.4 Modeling a Prior Belief of Exchangeability 7.5 Simulating from the Posterior 7.6 Posterior Inferences 7.7 Bayesian Sensitivity Analysis 7.8 Posterior Predictive Model Checking", " Chapter 7 Hierarchical Modeling 7.1 Introduction to Hierarchical Modeling library(LearnBayes) library(lattice) Fit logistic model for home run data for a particular player logistic.fit &lt;- function(player){ d &lt;- subset(sluggerdata, Player==player) x &lt;- d$Age x2 &lt;- d$Age^2 response &lt;- cbind(d$HR, d$AB - d$HR) list(Age=x, p=glm(response ~ x + x2, family=binomial)$fitted) } names &lt;- unique(sluggerdata$Player) newdata &lt;- NULL for (j in 1:9){ fit &lt;-logistic.fit(as.character(names[j])) newdata &lt;- rbind(newdata, data.frame(as.character(names[j]), fit$Age, fit$p)) } names(newdata) &lt;- c(&quot;Player&quot;, &quot;Age&quot;, &quot;Fitted&quot;) xyplot(Fitted ~ Age | Player, data=newdata, type=&quot;l&quot;, lwd=3, col=&quot;black&quot;) 7.2 Individual or Combined Estimates with(hearttransplants, plot(log(e), y / e, xlim=c(6, 9.7), xlab=&quot;log(e)&quot;, ylab=&quot;y/e&quot;)) with(hearttransplants, text(log(e), y / e, labels=as.character(y), pos=4)) 7.3 Equal Mortality Rates? Using posterior predictive checks to see if equal mortality rate model is appropriate. with(hearttransplants, sum(y)) ## [1] 277 with(hearttransplants, sum(e)) ## [1] 294681 lambda &lt;- rgamma(1000, shape=277, rate=294681) ys94 &lt;- with(hearttransplants, rpois(1000, e[94] * lambda)) hist(ys94, breaks=seq(0.5, max(ys94) + 0.5)) with(hearttransplants, lines(c(y[94], y[94]), c(0, 120), lwd=3)) Find posterior predictive distribution of each observation with its posterior predictive distribution. lambda &lt;- rgamma(1000, shape=277, rate=294681) prob.out &lt;- function(i){ ysi &lt;- with(hearttransplants, rpois(1000, e[i] * lambda)) pleft &lt;- with(hearttransplants, sum(ysi &lt;= y[i]) / 1000) pright &lt;- with(hearttransplants, sum(ysi &gt;= y[i]) / 1000) min(pleft, pright) } pout &lt;- sapply(1:94, prob.out) with(hearttransplants, plot(log(e), pout, ylab=&quot;Prob(extreme)&quot;)) 7.4 Modeling a Prior Belief of Exchangeability Graph of two-stage prior to model a belief in exchangeability of the Poisson rates. pgexchprior &lt;- function(lambda, pars){ alpha &lt;- pars[1] a &lt;- pars[2] b &lt;- pars[3] (alpha - 1) * log(prod(lambda)) - (2 * alpha + a) * log(alpha * sum(lambda) + b) } alpha &lt;- c(5, 20, 80, 400) par(mfrow=c(2, 2)) for (j in 1:4){ mycontour(pgexchprior, c(.001, 5, .001, 5), c(alpha[j], 10, 10), main=paste(&quot;ALPHA = &quot;,alpha[j]), xlab=&quot;LAMBDA 1&quot;, ylab=&quot;LAMBDA 2&quot;) } 7.5 Simulating from the Posterior Representing posterior as [\\(\\mu, \\alpha\\)] [\\(\\{\\lambda_j\\} | \\mu, \\alpha\\)]. Focus on posterior of [\\(\\mu, \\alpha\\)]: datapar &lt;- list(data = hearttransplants, z0 = 0.53) start &lt;- c(2, -7) fit &lt;- laplace(poissgamexch, start, datapar) fit ## $mode ## [1] 1.883954 -6.955446 ## ## $var ## [,1] [,2] ## [1,] 0.233694921 -0.003086655 ## [2,] -0.003086655 0.005866020 ## ## $int ## [1] -2208.503 ## ## $converge ## [1] TRUE par(mfrow = c(1, 1)) mycontour(poissgamexch, c(0, 8, -7.3, -6.6), datapar, xlab=&quot;log alpha&quot;, ylab=&quot;log mu&quot;) start &lt;- c(4, -7) fitgibbs &lt;- gibbs(poissgamexch, start, 1000, c(1, .15), datapar) fitgibbs$accept ## [,1] [,2] ## [1,] 0.52 0.483 mycontour(poissgamexch, c(0, 8, -7.3, -6.6), datapar, xlab=&quot;log alpha&quot;, ylab=&quot;log mu&quot;) points(fitgibbs$par[, 1], fitgibbs$par[, 2]) plot(density(fitgibbs$par[, 1], bw = 0.2)) Posterior of rates: alpha &lt;- exp(fitgibbs$par[, 1]) mu &lt;- exp(fitgibbs$par[, 2]) lam1 &lt;- rgamma(1000, y[1] + alpha, hearttransplants$e[1] + alpha / mu) alpha &lt;- exp(fitgibbs$par[, 1]) mu &lt;- exp(fitgibbs$par[, 2]) with(hearttransplants, plot(log(e), y/e, pch = as.character(y))) for (i in 1:94) { lami &lt;- with(hearttransplants, rgamma(1000, y[i] + alpha, e[i] + alpha/mu)) probint &lt;- quantile(lami, c(0.05, 0.95)) with(hearttransplants, lines(log(e[i]) * c(1, 1), probint)) } 7.6 Posterior Inferences datapar &lt;- list(data = hearttransplants, z0 = 0.53) start &lt;- c(2, -7) fit &lt;- laplace(poissgamexch, start, datapar) fit ## $mode ## [1] 1.883954 -6.955446 ## ## $var ## [,1] [,2] ## [1,] 0.233694921 -0.003086655 ## [2,] -0.003086655 0.005866020 ## ## $int ## [1] -2208.503 ## ## $converge ## [1] TRUE par(mfrow = c(1, 1)) mycontour(poissgamexch, c(0, 8, -7.3, -6.6), datapar, xlab=&quot;log alpha&quot;,ylab=&quot;log mu&quot;) start &lt;- c(4, -7) fitgibbs &lt;- gibbs(poissgamexch, start, 1000, c(1,.15), datapar) alpha &lt;- exp(fitgibbs$par[, 1]) mu &lt;- exp(fitgibbs$par[, 2]) Look at posteriors of shrinkages. shrink &lt;-function(i) with(hearttransplants, mean(alpha / (alpha + e[i] * mu))) shrinkage=sapply(1:94, shrink) with(hearttransplants, plot(log(e), shrinkage)) Comparing hospitals. mrate &lt;- function(i){ with(hearttransplants, mean(rgamma(1000, y[i] + alpha, e[i] + alpha/mu))) } hospital &lt;- 1:94 meanrate &lt;- sapply(hospital,mrate) hospital[meanrate == min(meanrate)] ## [1] 85 sim.lambda &lt;- function(i) { with(hearttransplants, rgamma(1000, y[i] + alpha, e[i] + alpha / mu)) } LAM &lt;- sapply(1:94, sim.lambda) compare.rates &lt;- function(x) { nc &lt;- NCOL(x) ij &lt;- as.matrix(expand.grid(1:nc, 1:nc)) m &lt;- as.matrix(x[,ij[,1]] &gt; x[,ij[,2]]) matrix(colMeans(m), nc, nc, byrow = TRUE) } better &lt;- compare.rates(LAM) better[1:24, 85] ## [1] 0.174 0.190 0.086 0.144 0.124 0.227 0.203 0.166 0.067 0.255 0.196 0.176 ## [13] 0.202 0.097 0.070 0.231 0.239 0.088 0.258 0.185 0.166 0.152 0.057 0.087 7.7 Bayesian Sensitivity Analysis Explore sensitivity of inference with respect to the choice of \\(z_0\\) in prior. datapar &lt;- list(data = hearttransplants, z0 = 0.53) start &lt;- c(4, -7) fitgibbs &lt;-gibbs(poissgamexch, start, 1000, c(1,.15), datapar) sir.old.new &lt;- function(theta, prior, prior.new){ log.g &lt;- log(prior(theta)) log.g.new &lt;- log(prior.new(theta)) wt &lt;- exp(log.g.new - log.g - max(log.g.new - log.g)) probs &lt;- wt / sum(wt) n &lt;- length(probs) indices &lt;- sample(1:n, size=n, prob=probs, replace=TRUE) theta[indices] } prior &lt;- function(theta){ 0.53 * exp(theta) / (exp(theta) + 0.53) ^ 2 } prior.new &lt;- function(theta){ 5 * exp(theta) / (exp(theta) + 5) ^ 2 } log.alpha &lt;- fitgibbs$par[, 1] log.alpha.new &lt;- sir.old.new(log.alpha, prior, prior.new) library(lattice) draw.graph &lt;- function(){ LOG.ALPHA &lt;- data.frame(&quot;prior&quot;, log.alpha) names(LOG.ALPHA) &lt;- c(&quot;Prior&quot;, &quot;log.alpha&quot;) LOG.ALPHA.NEW &lt;- data.frame(&quot;new.prior&quot;, log.alpha.new) names(LOG.ALPHA.NEW) &lt;- c(&quot;Prior&quot;,&quot;log.alpha&quot;) D &lt;- densityplot(~ log.alpha, group=Prior, data = rbind(LOG.ALPHA, LOG.ALPHA.NEW), plot.points=FALSE, main=&quot;Original Prior and Posterior (solid), \\nNew Prior and Posterior (dashed)&quot;, lwd=4, adjust=2, lty=c(1,2), xlab=&quot;log alpha&quot;,xlim=c(-3,5),col=&quot;black&quot;) update(D, panel=function(...){ panel.curve(prior(x), lty=1, lwd=2, col=&quot;black&quot;) panel.curve(prior.new(x), lty=2, lwd=2, col=&quot;black&quot;) panel.densityplot(...) })} draw.graph() 7.8 Posterior Predictive Model Checking Study predictive distributions of observations. datapar &lt;- list(data = hearttransplants, z0 = 0.53) start &lt;- c(4, -7) fitgibbs &lt;- gibbs(poissgamexch, start, 1000, c(1,.15), datapar) lam94 &lt;- with(hearttransplants, rgamma(1000, y[94] + alpha, e[94] + alpha / mu)) ys94 &lt;- with(hearttransplants, rpois(1000, e[94] * lam94)) hist(ys94, breaks=seq(-0.5, max(ys94) + 0.5)) lines(y[94] * c(1, 1), c(0, 100), lwd=3) Explore the probabilities that the predictive distribution of each observation is at least as large as observed \\(y_i\\). prob.out &lt;- function(i){ lami &lt;- with(hearttransplants, rgamma(1000, y[i] + alpha, e[i] + alpha / mu)) ysi &lt;- with(hearttransplants, rpois(1000, e[i] * lami)) pleft &lt;- with(hearttransplants, sum(ysi &lt;= y[i]) / 1000) pright &lt;- with(hearttransplants, sum(ysi &gt;= y[i]) / 1000) min(pleft, pright) } pout.exchange &lt;- sapply(1:94, prob.out) plot(pout, pout.exchange, xlab=&quot;P(extreme), equal means&quot;, ylab=&quot;P(extreme), exchangeable&quot;) abline(0,1) "],["model-comparison.html", "Chapter 8 Model Comparison 8.1 A One-Sided Test of a Normal Mean 8.2 A Two-Sided Test of a Normal Mean 8.3 Models for Soccer Goals 8.4 Is a Baseball Hitter Really Streaky? 8.5 A Test of Independence in a Two-Way Contingency Table", " Chapter 8 Model Comparison 8.1 A One-Sided Test of a Normal Mean Bayesian testing of \\(\\mu \\le \\mu_0\\) against \\(\\mu &gt; \\mu_0\\). library(LearnBayes) pmean &lt;- 170 pvar &lt;- 25 probH &lt;- pnorm(175, pmean, sqrt(pvar)) probA &lt;- 1 - probH prior.odds &lt;- probH / probA prior.odds ## [1] 5.302974 weights &lt;- c(182, 172, 173, 176, 176, 180, 173, 174, 179, 175) xbar &lt;- mean(weights) sigma2 &lt;- 3 ^ 2 / length(weights) post.precision &lt;- 1 / sigma2 + 1 / pvar post.var &lt;- 1 / post.precision post.mean &lt;- (xbar / sigma2 + pmean / pvar) / post.precision c(post.mean, sqrt(post.var)) ## [1] 175.7915058 0.9320546 post.odds &lt;- pnorm(175, post.mean, sqrt(post.var)) / (1 - pnorm(175, post.mean, sqrt(post.var))) post.odds ## [1] 0.2467017 BF &lt;- post.odds / prior.odds BF ## [1] 0.04652139 postH &lt;- probH * BF / (probH * BF + probA) postH ## [1] 0.1978835 Contrast with a frequentist p-value calculation. z &lt;- sqrt(length(weights)) * (mean(weights) - 175) / 3 1 - pnorm(z) ## [1] 0.1459203 weights &lt;- c(182, 172, 173, 176, 176, 180, 173, 174, 179, 175) data &lt;- c(mean(weights), length(weights), 3) prior.par &lt;- c(170, 1000) mnormt.onesided(175, prior.par, data) ## $BF ## [1] 0.1694947 ## ## $prior.odds ## [1] 1.008011 ## ## $post.odds ## [1] 0.1708525 ## ## $postH ## [1] 0.1459215 8.2 A Two-Sided Test of a Normal Mean Bayesian testing of \\(\\mu = \\mu_0\\) against \\(\\mu \\neq \\mu_0\\). weights &lt;- c(182, 172, 173, 176, 176, 180, 173, 174, 179, 175) data &lt;- c(mean(weights), length(weights), 3) t &lt;- c(.5, 1, 2, 4, 8) mnormt.twosided(170, .5, t, data) ## $bf ## [1] 1.462146e-02 3.897038e-05 1.894326e-07 2.591162e-08 2.309739e-08 ## ## $post ## [1] 1.441076e-02 3.896887e-05 1.894325e-07 2.591162e-08 2.309739e-08 8.3 Models for Soccer Goals Illustrates the use of the marginal likelihood to compare several Bayesian models for soccer goals. datapar &lt;- list(data=soccergoals$goals, par=c(4.57, 1.43)) fit1 &lt;- laplace(logpoissgamma, .5, datapar) datapar &lt;- list(data=soccergoals$goals, par=c(1, .5)) fit2 &lt;- laplace(logpoissnormal, .5, datapar) datapar &lt;- list(data=soccergoals$goals, par=c(2, .5)) fit3 &lt;- laplace(logpoissnormal, .5, datapar) datapar &lt;- list(data=soccergoals$goals, par=c(1, 2)) fit4 &lt;- laplace(logpoissnormal, .5, datapar) postmode &lt;- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode) postsd &lt;- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var)) logmarg &lt;- c(fit1$int, fit2$int, fit3$int, fit4$int) cbind(postmode,postsd,logmarg) ## postmode postsd logmarg ## [1,] 0.5248047 0.1274414 -1.502977 ## [2,] 0.5207825 0.1260712 -1.255171 ## [3,] 0.5825195 0.1224723 -5.076316 ## [4,] 0.4899414 0.1320165 -2.137216 8.4 Is a Baseball Hitter Really Streaky? Defines a family of streaky models to measure the level of support for streakiness by a Bayes factor. data &lt;- cbind(jeter2004$H, jeter2004$AB) data1 &lt;- regroup(data, 5) log.marg &lt;- function(logK){ laplace(bfexch, 0, list(data=data1, K=exp(logK)))$int } log.K &lt;- seq(2, 6) K &lt;- exp(log.K) log.BF &lt;- sapply(log.K, log.marg) BF &lt;- exp(log.BF) round(data.frame(log.K, K, log.BF, BF), 2) ## log.K K log.BF BF ## 1 2 7.39 -4.04 0.02 ## 2 3 20.09 0.17 1.19 ## 3 4 54.60 0.92 2.51 ## 4 5 148.41 0.57 1.78 ## 5 6 403.43 0.26 1.29 8.5 A Test of Independence in a Two-Way Contingency Table Constructs several Bayes factor statistics for two-way contingency tables. data &lt;- matrix(c(11, 9, 68, 23, 3, 5), c(2, 3)) data ## [,1] [,2] [,3] ## [1,] 11 68 3 ## [2,] 9 23 5 Traditional chi-square test of independence. chisq.test(data) ## Warning in chisq.test(data): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 6.9264, df = 2, p-value = 0.03133 Bayes factor against independence using uniform priors. a=matrix(rep(1, 6), c(2, 3)) a ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ctable(data, a) ## [1] 1.662173 Consider Bayes factors against independence for alternatives close to independence. log.K &lt;- seq(2,7) compute.log.BF &lt;- function(log.K){ log(bfindep(data, exp(log.K), 100000)$bf) } log.BF &lt;- sapply(log.K, compute.log.BF) BF &lt;- exp(log.BF) round(data.frame(log.K, log.BF, BF), 2) ## log.K log.BF BF ## 1 2 -1.51 0.22 ## 2 3 0.13 1.14 ## 3 4 0.78 2.17 ## 4 5 0.73 2.09 ## 5 6 0.44 1.55 ## 6 7 0.20 1.22 "],["regression-models.html", "Chapter 9 Regression Models 9.1 An Example of Bayesian Regression 9.2 Modeling Using Zellner’s g Prior 9.3 Survival Modeling", " Chapter 9 Regression Models 9.1 An Example of Bayesian Regression library(LearnBayes) logtime &lt;- log(birdextinct$time) plot(birdextinct$nesting, logtime) out &lt;- (logtime &gt; 3) text(birdextinct$nesting[out], logtime[out], label=birdextinct$species[out], pos = 2) plot(jitter(birdextinct$size), logtime, xaxp=c(0, 1, 1)) plot(jitter(birdextinct$status), logtime, xaxp=c(0, 1, 1)) Least-squares fit: fit &lt;- lm(logtime ~ nesting + size + status, data=birdextinct, x=TRUE, y=TRUE) summary(fit) ## ## Call: ## lm(formula = logtime ~ nesting + size + status, data = birdextinct, ## x = TRUE, y = TRUE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8410 -0.2932 -0.0709 0.2165 2.5167 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.43087 0.20706 2.081 0.041870 * ## nesting 0.26501 0.03679 7.203 1.33e-09 *** ## size -0.65220 0.16667 -3.913 0.000242 *** ## status 0.50417 0.18263 2.761 0.007712 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6524 on 58 degrees of freedom ## Multiple R-squared: 0.5982, Adjusted R-squared: 0.5775 ## F-statistic: 28.79 on 3 and 58 DF, p-value: 1.577e-11 Sampling from posterior using vague priors for parameters. theta.sample &lt;- blinreg(fit$y, fit$x, 5000) par(mfrow=c(2,2)) hist(theta.sample$beta[,2], main=&quot;NESTING&quot;, xlab=expression(beta[1])) hist(theta.sample$beta[,3], main=&quot;SIZE&quot;, xlab=expression(beta[2])) hist(theta.sample$beta[,4], main=&quot;STATUS&quot;, xlab=expression(beta[3])) hist(theta.sample$sigma, main=&quot;ERROR SD&quot;, xlab=expression(sigma)) apply(theta.sample$beta, 2, quantile, c(.05, .5, .95)) ## X(Intercept) Xnesting Xsize Xstatus ## 5% 0.08301736 0.2031766 -0.9385281 0.1871552 ## 50% 0.42537546 0.2639260 -0.6541445 0.5055329 ## 95% 0.79519902 0.3258697 -0.3713111 0.8134480 quantile(theta.sample$sigma, c(.05, .5, .95)) ## 5% 50% 95% ## 0.5667969 0.6573449 0.7703726 Estimating mean extinction times: cov1 &lt;- c(1, 4, 0, 0) cov2 &lt;- c(1, 4, 1, 0) cov3 &lt;- c(1, 4, 0, 1) cov4 &lt;- c(1, 4, 1, 1) X1 &lt;- rbind(cov1, cov2, cov3, cov4) mean.draws &lt;- blinregexpected(X1, theta.sample) c.labels &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) par(mfrow=c(2, 2)) for (j in 1:4){ hist(mean.draws[, j], main=paste(&quot;Covariate set&quot;, c.labels[j]), xlab=&quot;log TIME&quot;) } Predicting future extinction times: cov1 &lt;- c(1, 4, 0, 0) cov2 &lt;- c(1, 4, 1, 0) cov3 &lt;- c(1, 4, 0, 1) cov4 &lt;- c(1, 4, 1, 1) X1 &lt;- rbind(cov1, cov2, cov3, cov4) pred.draws &lt;- blinregpred(X1, theta.sample) c.labels &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) par(mfrow=c(2,2)) for (j in 1:4){ hist(pred.draws[, j], main=paste(&quot;Covariate set&quot;, c.labels[j]), xlab=&quot;log TIME&quot;) } Model checking: posterior predictive distribution distributions of each future observation, showing actual observation as solid dot. pred.draws &lt;- blinregpred(fit$x, theta.sample) pred.sum &lt;- apply(pred.draws, 2, quantile, c(.05,.95)) par(mfrow=c(1, 1)) ind &lt;- 1:length(logtime) matplot(rbind(ind, ind), pred.sum, type=&quot;l&quot;, lty=1, col=1, xlab=&quot;INDEX&quot;, ylab=&quot;log TIME&quot;) points(ind, logtime, pch=19) out &lt;- (logtime &gt; pred.sum[2, ]) text(ind[out], logtime[out], label=birdextinct$species[out], pos = 4) Model checking via bayes residuals \\(y_i - x_i \\beta\\). Graph of absolute values of residuals that exceeds a particular constant. prob.out &lt;- bayesresiduals(fit, theta.sample, 2) par(mfrow=c(1, 1)) plot(birdextinct$nesting, prob.out) out = (prob.out &gt; 0.35) text(birdextinct$nesting[out], prob.out[out], label=birdextinct$species[out], pos = 4) 9.2 Modeling Using Zellner’s g Prior Illustrating the role of the parameter c: X &lt;- cbind(1, puffin$Distance - mean(puffin$Distance)) c.prior &lt;- c(0.1, 0.5, 5, 2) fit &lt;- vector(&quot;list&quot;, 4) for (j in 1:4){ prior &lt;- list(b0=c(8, 0), c0=c.prior[j]) fit[[j]] &lt;- blinreg(puffin$Nest, X, 1000, prior) } BETA &lt;- NULL for (j in 1:4){ s=data.frame(Prior=paste(&quot;c =&quot;, as.character(c.prior[j])), beta0=fit[[j]]$beta[, 1], beta1=fit[[j]]$beta[, 2]) BETA &lt;- rbind(BETA, s) } library(lattice) with(BETA, xyplot(beta1 ~ beta0 | Prior, type=c(&quot;p&quot;,&quot;g&quot;), col=&quot;black&quot;)) Model selection of all regression models using g priors: data &lt;- list(y=puffin$Nest, X=cbind(1, puffin$Grass, puffin$Soil)) prior &lt;- list(b0=c(0, 0, 0), c0=100) beta.start &lt;- with(puffin, lm(Nest ~ Grass + Soil)$coef) laplace(reg.gprior.post, c(beta.start, 0), list(data=data, prior=prior))$int ## [1] -136.3957 X &lt;- puffin[, -1] y &lt;- puffin$Nest c &lt;- 100 bayes.model.selection(y, X, c, constant=FALSE) ## $mod.prob ## Grass Soil Angle Distance log.m Prob ## 1 FALSE FALSE FALSE FALSE -132.18 0.00000 ## 2 TRUE FALSE FALSE FALSE -134.05 0.00000 ## 3 FALSE TRUE FALSE FALSE -134.51 0.00000 ## 4 TRUE TRUE FALSE FALSE -136.40 0.00000 ## 5 FALSE FALSE TRUE FALSE -112.67 0.00000 ## 6 TRUE FALSE TRUE FALSE -113.18 0.00000 ## 7 FALSE TRUE TRUE FALSE -114.96 0.00000 ## 8 TRUE TRUE TRUE FALSE -115.40 0.00000 ## 9 FALSE FALSE FALSE TRUE -103.30 0.03500 ## 10 TRUE FALSE FALSE TRUE -105.57 0.00360 ## 11 FALSE TRUE FALSE TRUE -100.37 0.65065 ## 12 TRUE TRUE FALSE TRUE -102.35 0.08992 ## 13 FALSE FALSE TRUE TRUE -102.81 0.05682 ## 14 TRUE FALSE TRUE TRUE -105.09 0.00581 ## 15 FALSE TRUE TRUE TRUE -101.88 0.14386 ## 16 TRUE TRUE TRUE TRUE -104.19 0.01434 ## ## $converge ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [16] TRUE 9.3 Survival Modeling Traditional fit using a Weibull model: library(survival) survreg(Surv(time, status) ~ factor(treat) + age, dist=&quot;weibull&quot;, data = chemotherapy) ## Call: ## survreg(formula = Surv(time, status) ~ factor(treat) + age, data = chemotherapy, ## dist = &quot;weibull&quot;) ## ## Coefficients: ## (Intercept) factor(treat)2 age ## 10.98683919 0.56145663 -0.07897718 ## ## Scale= 0.5489202 ## ## Loglik(model)= -88.7 Loglik(intercept only)= -98 ## Chisq= 18.41 on 2 degrees of freedom, p= 0.000101 ## n= 26 Bayesian fit: start &lt;- c(-.5, 9, .5, -.05) d &lt;- with(chemotherapy, cbind(time, status, treat - 1, age)) fit &lt;- laplace(weibullregpost, start, d) fit ## $mode ## [1] -0.59986796 10.98663371 0.56151088 -0.07897316 ## ## $var ## [,1] [,2] [,3] [,4] ## [1,] 0.057298875 0.13530436 0.004541435 -0.0020828431 ## [2,] 0.135304360 1.67428176 -0.156631948 -0.0255278352 ## [3,] 0.004541435 -0.15663195 0.115450201 0.0017880712 ## [4,] -0.002082843 -0.02552784 0.001788071 0.0003995202 ## ## $int ## [1] -25.31207 ## ## $converge ## [1] TRUE proposal &lt;- list(var=fit$var, scale=1.5) bayesfit &lt;- rwmetrop(weibullregpost, proposal, fit$mode, 10000, d) bayesfit$accept ## [1] 0.2849 par(mfrow=c(2, 2)) sigma &lt;- exp(bayesfit$par[, 1]) mu &lt;- bayesfit$par[, 2] beta1 &lt;- bayesfit$par[, 3] beta2 &lt;- bayesfit$par[, 4] hist(beta1, xlab=&quot;treatment&quot;, main=&quot;&quot;) hist(beta2, xlab=&quot;age&quot;, main=&quot;&quot;) hist(sigma, xlab=&quot;sigma&quot;, main=&quot;&quot;) "],["gibbs-sampling.html", "Chapter 10 Gibbs Sampling 10.1 Robust Modeling 10.2 Binary Response Regression with a Probit Link 10.3 Estimating a Table of Means", " Chapter 10 Gibbs Sampling 10.1 Robust Modeling Illustrating Gibbs sampling using a t sampling model. library(LearnBayes) fit &lt;- robustt(darwin$difference, 4, 10000) plot(density(fit$mu), xlab=&quot;mu&quot;) The \\(\\lambda_j\\) parameters indicate the outlying observations. mean.lambda &lt;- apply(fit$lam, 2, mean) lam5 &lt;- apply(fit$lam, 2, quantile, .05) lam95 &lt;- apply(fit$lam, 2, quantile, .95) plot(darwin$difference, mean.lambda, lwd=2, ylim=c(0,3), ylab=&quot;Lambda&quot;) for (i in 1:length(darwin$difference)){ lines(c(1, 1) * darwin$difference[i], c(lam5[i], lam95[i])) } points(darwin$difference, 0 * darwin$difference-.05, pch=19, cex=2) 10.2 Binary Response Regression with a Probit Link Missing data and Gibbs sampling X &lt;- with(donner, cbind(1, age, male)) Traditional probit fit: fit &lt;- glm(survival ~ X - 1, family=binomial(link=probit), data = donner) summary(fit) ## ## Call: ## glm(formula = survival ~ X - 1, family = binomial(link = probit), ## data = donner) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7420 -1.0555 -0.2756 0.8861 2.0339 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## X 1.91730 0.76438 2.508 0.0121 * ## Xage -0.04571 0.02076 -2.202 0.0277 * ## Xmale -0.95828 0.43983 -2.179 0.0293 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 62.383 on 45 degrees of freedom ## Residual deviance: 51.283 on 42 degrees of freedom ## AIC: 57.283 ## ## Number of Fisher Scoring iterations: 5 Bayesian fit of the probit model using data augmentation. m &lt;- 10000 fit &lt;- bayes.probit(donner$survival, X, m) apply(fit$beta,2,mean) ## [1] 2.06883244 -0.05003696 -1.00001104 apply(fit$beta,2,sd) ## [1] 0.80049309 0.02133692 0.46412859 Posterior distributions of specific probabilities. a &lt;- seq(15, 65) X1 &lt;- cbind(1, a, 1) p.male &lt;- bprobit.probs(X1, fit$beta) plot(a, apply(p.male, 2, quantile, .5), type=&quot;l&quot;, ylim=c(0,1), xlab=&quot;age&quot;, ylab=&quot;Probability of Survival&quot;) lines(a,apply(p.male, 2, quantile, .05), lty=2) lines(a,apply(p.male, 2, quantile, .95), lty=2) Proper priors and model selection of probit models. y &lt;- donner$survival X &lt;- cbind(1, donner$age, donner$male) beta0 &lt;- c(0,0,0) c0 &lt;- 100 P0 &lt;- t(X) %*% X / c0 bayes.probit(y, X, 1000, list(beta=beta0, P=P0))$log.marg ## [1] -31.529 bayes.probit(y, X[, -2], 1000, list(beta=beta0[-2], P=P0[-2, -2]))$log.marg ## [1] -32.75655 bayes.probit(y, X[, -3], 1000, list(beta=beta0[-3], P=P0[-3, -3]))$log.marg ## [1] -31.99984 bayes.probit(y, X[, -c(2, 3)], 1000, list(beta=beta0[- c(2, 3)], P=P0[-c(2, 3), -c(2, 3)]))$log.marg ## [1] -32.98301 10.3 Estimating a Table of Means rlabels &lt;- c(&quot;91-99&quot;, &quot;81-90&quot;, &quot;71-80&quot;, &quot;61-70&quot;, &quot;51-60&quot;, &quot;41-50&quot;, &quot;31-40&quot;, &quot;21-30&quot;) clabels &lt;- c(&quot;16-18&quot;, &quot;19-21&quot;, &quot;22-24&quot;, &quot;25-27&quot;, &quot;28-30&quot;) gpa &lt;- matrix(iowagpa[, 1], nrow = 8, ncol = 5, byrow = T) dimnames(gpa) &lt;- list(HSR = rlabels, ACTC = clabels) gpa ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 2.64 3.10 3.01 3.07 3.34 ## 81-90 2.24 2.63 2.74 2.76 2.91 ## 71-80 2.43 2.47 2.64 2.73 2.47 ## 61-70 2.31 2.37 2.32 2.24 2.31 ## 51-60 2.04 2.20 2.01 2.43 2.38 ## 41-50 1.88 1.82 1.84 2.12 2.05 ## 31-40 1.86 2.28 1.67 1.89 1.79 ## 21-30 1.70 1.65 1.51 1.67 2.33 samplesizes &lt;- matrix(iowagpa[, 2], nrow = 8, ncol = 5, byrow = T) dimnames(samplesizes) &lt;- list(HSR = rlabels, ACTC = clabels) samplesizes ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 8 15 78 182 166 ## 81-90 20 71 168 178 91 ## 71-80 40 116 180 133 46 ## 61-70 34 93 124 101 19 ## 51-60 41 73 62 58 9 ## 41-50 19 25 36 49 16 ## 31-40 8 9 15 29 9 ## 21-30 4 5 9 11 1 act &lt;- seq(17, 29, by = 3) matplot(act, t(gpa), type = &quot;l&quot;, lwd = 3, xlim = c(17, 34), col=1:8, lty=1:8) legend(30, 3, lty = 1:8, lwd = 3, legend = c(&quot;HSR=9&quot;, &quot;HSR=8&quot;, &quot;HSR=7&quot;, &quot;HSR=6&quot;, &quot;HSR=5&quot;, &quot;HSR=4&quot;, &quot;HSR=3&quot;, &quot;HSR=2&quot;), col=1:8) Fitting a Bayesian model with a flat prior over the restricted space. MU &lt;- ordergibbs(iowagpa, 5000) postmeans &lt;- apply(MU, 2, mean) postmeans &lt;- matrix(postmeans, nrow = 8, ncol = 5) postmeans &lt;- postmeans[seq(8, 1, -1), ] dimnames(postmeans) &lt;- list(HSR=rlabels, ACTC=clabels) round(postmeans, 2) ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 2.65 2.92 3.01 3.09 3.34 ## 81-90 2.41 2.62 2.73 2.78 2.92 ## 71-80 2.33 2.47 2.62 2.68 2.71 ## 61-70 2.20 2.29 2.33 2.37 2.50 ## 51-60 1.99 2.11 2.15 2.31 2.41 ## 41-50 1.76 1.86 1.94 2.10 2.21 ## 31-40 1.58 1.74 1.81 1.91 2.05 ## 21-30 1.23 1.42 1.55 1.69 1.88 matplot(act, t(postmeans), type = &quot;l&quot;, lty=1:8, lwd = 3, col = 1, xlim = c(17, 34)) legend(30, 3, lty = 1:8, lwd = 2, legend = c(&quot;HSR=9&quot;, &quot;HSR=8&quot;, &quot;HSR=7&quot;, &quot;HSR=6&quot;, &quot;HSR=5&quot;, &quot;HSR=4&quot;, &quot;HSR=3&quot;, &quot;HSR=2&quot;)) postsds &lt;- apply(MU, 2, sd) postsds &lt;- matrix(postsds, nrow = 8, ncol = 5) postsds &lt;- postsds[seq(8, 1, -1), ] dimnames(postsds) &lt;- list(HSR=rlabels, ACTC=clabels) round(postsds, 3) ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 0.141 0.084 0.054 0.043 0.050 ## 81-90 0.077 0.058 0.038 0.038 0.062 ## 71-80 0.065 0.053 0.038 0.039 0.048 ## 61-70 0.065 0.039 0.036 0.039 0.081 ## 51-60 0.074 0.052 0.053 0.049 0.073 ## 41-50 0.082 0.068 0.067 0.070 0.087 ## 31-40 0.115 0.077 0.070 0.074 0.096 ## 21-30 0.177 0.136 0.118 0.113 0.131 s &lt;- .65 se &lt;- s / sqrt(samplesizes) round(postsds / se, 2) ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 0.61 0.50 0.74 0.90 1.00 ## 81-90 0.53 0.75 0.75 0.79 0.91 ## 71-80 0.63 0.88 0.79 0.69 0.50 ## 61-70 0.58 0.58 0.62 0.61 0.54 ## 51-60 0.73 0.68 0.65 0.58 0.34 ## 41-50 0.55 0.52 0.62 0.76 0.53 ## 31-40 0.50 0.36 0.42 0.61 0.44 ## 21-30 0.55 0.47 0.54 0.58 0.20 Fit of a hierarchical regression prior: FIT &lt;- hiergibbs(iowagpa, 5000) par(mfrow=c(2,1)) plot(density(FIT$beta[, 2]), xlab=expression(beta[2]), main=&quot;HIGH SCHOOL RANK&quot;) plot(density(FIT$beta[, 3]), xlab=expression(beta[3]), main=&quot;ACT SCORE&quot;) quantile(FIT$beta[, 2], c(.025, .25, .5, .75, .975)) ## 2.5% 25% 50% 75% 97.5% ## 0.01793738 0.01881165 0.01924078 0.01968339 0.02051583 quantile(FIT$beta[, 3], c(.025, .25, .5, .75, .975)) ## 2.5% 25% 50% 75% 97.5% ## 0.02209046 0.02627562 0.02845516 0.03053914 0.03466108 quantile(FIT$var, c(.025, .25, .5, .75, .975)) ## 2.5% 25% 50% 75% 97.5% ## 0.001104009 0.002020655 0.002924627 0.004204618 0.007893763 posterior.means &lt;- apply(FIT$mu, 2, mean) posterior.means &lt;- matrix(posterior.means, nrow = 8, ncol = 5, byrow = T) par(mfrow=c(1, 1)) matplot(act, t(posterior.means), type = &quot;l&quot;, lwd = 3, lty=1:8, col=1, xlim = c(17, 34)) legend(30, 3, lty = 1:8, lwd = 2, legend = c(&quot;HSR=9&quot;, &quot;HSR=8&quot;, &quot;HSR=7&quot;, &quot;HSR=6&quot;, &quot;HSR=5&quot;, &quot;HSR=4&quot;, &quot;HSR=3&quot;, &quot;HSR=2&quot;)) p &lt;- 1 - pnorm((2.5 - FIT$mu) / .65) prob.success &lt;- apply(p, 2, mean) prob.success &lt;- matrix(prob.success, nrow=8, ncol=5, byrow=T) dimnames(prob.success) &lt;- list(HSR=rlabels, ACTC=clabels) round(prob.success,3) ## ACTC ## HSR 16-18 19-21 22-24 25-27 28-30 ## 91-99 0.689 0.748 0.782 0.813 0.879 ## 81-90 0.554 0.617 0.662 0.689 0.757 ## 71-80 0.466 0.503 0.579 0.630 0.625 ## 61-70 0.360 0.410 0.425 0.438 0.537 ## 51-60 0.249 0.304 0.303 0.409 0.441 ## 41-50 0.168 0.193 0.221 0.282 0.320 ## 31-40 0.107 0.141 0.153 0.190 0.224 ## 21-30 0.062 0.078 0.096 0.121 0.153 "]]
